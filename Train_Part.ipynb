{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting datatools\n",
      "  Downloading https://files.pythonhosted.org/packages/39/1a/41111239f51eb3c75505c314af8c251acf49c806d20314eaf0d5526261b9/datatools-0.1.2.tar.gz\n",
      "Building wheels for collected packages: datatools\n",
      "  Building wheel for datatools (setup.py): started\n",
      "  Building wheel for datatools (setup.py): finished with status 'done'\n",
      "  Created wheel for datatools: filename=datatools-0.1.2-cp37-none-any.whl size=9427 sha256=ac1ff9c809cd6a47190deb1d6deedab9716d8ae07e1041262e3c95c20c04ee97\n",
      "  Stored in directory: C:\\Users\\74019\\AppData\\Local\\pip\\Cache\\wheels\\8e\\27\\d0\\85e0e5e87f42bab2df0f1fc9defe375b9fb12f0acb5a531c17\n",
      "Successfully built datatools\n",
      "Installing collected packages: datatools\n",
      "Successfully installed datatools-0.1.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install datatools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting librosa\n",
      "  Downloading https://files.pythonhosted.org/packages/77/b5/1817862d64a7c231afd15419d8418ae1f000742cac275e85c74b219cbccb/librosa-0.7.2.tar.gz (1.6MB)\n",
      "Collecting audioread>=2.0.0 (from librosa)\n",
      "  Downloading https://files.pythonhosted.org/packages/2e/0b/940ea7861e0e9049f09dcfd72a90c9ae55f697c17c299a323f0148f913d2/audioread-2.1.8.tar.gz\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\74019\\anaconda3\\lib\\site-packages (from librosa) (1.16.5)\n",
      "Requirement already satisfied: scipy>=1.0.0 in c:\\users\\74019\\anaconda3\\lib\\site-packages (from librosa) (1.4.1)\n",
      "Requirement already satisfied: scikit-learn!=0.19.0,>=0.14.0 in c:\\users\\74019\\anaconda3\\lib\\site-packages (from librosa) (0.22.2.post1)\n",
      "Requirement already satisfied: joblib>=0.12 in c:\\users\\74019\\anaconda3\\lib\\site-packages (from librosa) (0.13.2)\n",
      "Requirement already satisfied: decorator>=3.0.0 in c:\\users\\74019\\anaconda3\\lib\\site-packages (from librosa) (4.4.0)\n",
      "Requirement already satisfied: six>=1.3 in c:\\users\\74019\\anaconda3\\lib\\site-packages (from librosa) (1.12.0)\n",
      "Collecting resampy>=0.2.2 (from librosa)\n",
      "  Downloading https://files.pythonhosted.org/packages/79/75/e22272b9c2185fc8f3af6ce37229708b45e8b855fd4bc38b4d6b040fff65/resampy-0.2.2.tar.gz (323kB)\n",
      "Requirement already satisfied: numba>=0.43.0 in c:\\users\\74019\\anaconda3\\lib\\site-packages (from librosa) (0.45.1)\n",
      "Collecting soundfile>=0.9.0 (from librosa)\n",
      "  Downloading https://files.pythonhosted.org/packages/b8/de/24e4035f06540ebb4e9993238ede787063875b003e79c537511d32a74d29/SoundFile-0.10.3.post1-py2.py3.cp26.cp27.cp32.cp33.cp34.cp35.cp36.pp27.pp32.pp33-none-win_amd64.whl (689kB)\n",
      "Requirement already satisfied: llvmlite>=0.29.0dev0 in c:\\users\\74019\\anaconda3\\lib\\site-packages (from numba>=0.43.0->librosa) (0.29.0)\n",
      "Requirement already satisfied: cffi>=1.0 in c:\\users\\74019\\anaconda3\\lib\\site-packages (from soundfile>=0.9.0->librosa) (1.12.3)\n",
      "Requirement already satisfied: pycparser in c:\\users\\74019\\anaconda3\\lib\\site-packages (from cffi>=1.0->soundfile>=0.9.0->librosa) (2.19)\n",
      "Building wheels for collected packages: librosa, audioread, resampy\n",
      "  Building wheel for librosa (setup.py): started\n",
      "  Building wheel for librosa (setup.py): finished with status 'done'\n",
      "  Created wheel for librosa: filename=librosa-0.7.2-cp37-none-any.whl size=1612889 sha256=7a2733cd17cebe4229973ffb1aa1630f8add9a55d320da808c0417c22ebaeb1a\n",
      "  Stored in directory: C:\\Users\\74019\\AppData\\Local\\pip\\Cache\\wheels\\4c\\6e\\d7\\bb93911540d2d1e44d690a1561871e5b6af82b69e80938abef\n",
      "  Building wheel for audioread (setup.py): started\n",
      "  Building wheel for audioread (setup.py): finished with status 'done'\n",
      "  Created wheel for audioread: filename=audioread-2.1.8-cp37-none-any.whl size=23097 sha256=71b61d4e6e8939afed0e1d5eba9663ec49d8b6c87d0a9048a77629b250c2f47f\n",
      "  Stored in directory: C:\\Users\\74019\\AppData\\Local\\pip\\Cache\\wheels\\b9\\64\\09\\0b6417df9d8ba8bc61a7d2553c5cebd714ec169644c88fc012\n",
      "  Building wheel for resampy (setup.py): started\n",
      "  Building wheel for resampy (setup.py): finished with status 'done'\n",
      "  Created wheel for resampy: filename=resampy-0.2.2-cp37-none-any.whl size=320724 sha256=652eb7d66cd11e5d668c14e4dc98559654da102500fba71014a7eb86a18e30fb\n",
      "  Stored in directory: C:\\Users\\74019\\AppData\\Local\\pip\\Cache\\wheels\\fa\\c1\\56\\e0e12c6f7f3d2cdea9712b35136a2d40a7817c6210ec096485\n",
      "Successfully built librosa audioread resampy\n",
      "Installing collected packages: audioread, resampy, soundfile, librosa\n",
      "Successfully installed audioread-2.1.8 librosa-0.7.2 resampy-0.2.2 soundfile-0.10.3.post1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "import pickle\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.0\n"
     ]
    }
   ],
   "source": [
    "# Check that TF 2.1.0 is in use\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainmodel():\n",
    "    inputt = layers.Input((128,128,1))\n",
    "    conv1 = layers.Conv2D(filters=16, kernel_size=(3, 3), activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(inputt)\n",
    "    #conv1 = layers.LeakyReLU()(conv1)\n",
    "    conv2 = layers.Conv2D(filters=16, kernel_size=(3, 3), activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv1)\n",
    "    #conv1 = layers.LeakyReLU()(conv1)\n",
    "    pool1 = layers.MaxPooling2D(pool_size=(2, 2), strides=None, padding='valid', data_format=None)(conv2)\n",
    "    conv3 = layers.Conv2D(filters=32, kernel_size=(3, 3), activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool1)\n",
    "    #conv2 = layers.LeakyReLU()(conv2)\n",
    "    conv4 = layers.Conv2D(filters=32, kernel_size=(3, 3), activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv3)\n",
    "    #conv2 = layers.LeakyReLU()(conv2)\n",
    "    pool2 = layers.MaxPooling2D(pool_size=(2, 2), strides=None, padding='valid', data_format=None)(conv4)\n",
    "    conv5 = layers.Conv2D(filters=64, kernel_size=(3, 3), activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool2)\n",
    "    #conv3 = layers.LeakyReLU()(conv3)\n",
    "    conv6 = layers.Conv2D(filters=64, kernel_size=(3, 3), activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv5)\n",
    "    #conv3 = layers.LeakyReLU()(conv3)\n",
    "    pool3 = layers.MaxPooling2D(pool_size=(2, 2), strides=None, padding='valid', data_format=None)(conv6)\n",
    "    conv7 = layers.Conv2D(filters=128, kernel_size=(3, 3), activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool3)\n",
    "    #conv4 = layers.LeakyReLU()(conv4)\n",
    "    conv8 = layers.Conv2D(filters=128, kernel_size=(3, 3), activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv7)\n",
    "    #conv4 = layers.LeakyReLU()(conv4)\n",
    "    drop1 = layers.Dropout(0.5)(conv8)\n",
    "    pool4 = layers.MaxPooling2D(pool_size=(2, 2), strides=None, padding='valid', data_format=None)(drop1)\n",
    "\n",
    "    conv9 = layers.Conv2D(filters=256, kernel_size=(3, 3), activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool4)\n",
    "    #conv5 = layers.LeakyReLU()(conv5)\n",
    "    conv10 = layers.Conv2D(filters=256, kernel_size=(3, 3), activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv9)\n",
    "    #conv5 = layers.LeakyReLU()(conv5)\n",
    "    drop2 = layers.Dropout(0.5)(conv10)\n",
    "    \n",
    "    up1 = layers.UpSampling2D(size = (2, 2), data_format=None)(drop2)\n",
    "    conv11 = layers.Conv2D(filters=128, kernel_size=(2, 2), activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(up1)\n",
    "    #up6 = layers.LeakyReLU()(up6)\n",
    "    merge1 = layers.concatenate([drop1,conv11], axis = 3)\n",
    "    conv12 = layers.Conv2D(filters=128, kernel_size=(3, 3), activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge1)\n",
    "    #conv6 = layers.LeakyReLU()(conv6)\n",
    "    conv13 = layers.Conv2D(filters=128, kernel_size=(3, 3), activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv12)\n",
    "    #conv6 = layers.LeakyReLU()(conv6)\n",
    "    up2 = layers.UpSampling2D(size = (2, 2), data_format=None)(conv13)\n",
    "    conv14 = layers.Conv2D(filters=64, kernel_size=(2, 2), activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(up2)\n",
    "    #up7 = layers.LeakyReLU()(up7)\n",
    "    merge2 = layers.concatenate([conv6,conv14], axis = 3)\n",
    "    conv15 = layers.Conv2D(filters=64, kernel_size=(3, 3), activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge2)\n",
    "    #conv7 = layers.LeakyReLU()(conv7)\n",
    "    conv16 = layers.Conv2D(filters=64, kernel_size=(3, 3), activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv15)\n",
    "    #conv7 = layers.LeakyReLU()(conv7)\n",
    "    up3 = layers.UpSampling2D(size = (2, 2), data_format=None)(conv16)\n",
    "    conv17 = layers.Conv2D(filters=32, kernel_size=(2, 2), activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(up3)\n",
    "    #up8 = layers.LeakyReLU()(up8)\n",
    "    merge3 = layers.concatenate([conv4,conv17], axis = 3)\n",
    "    conv18 = layers.Conv2D(filters=32, kernel_size=(3, 3), activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge3)\n",
    "    #conv8 = layers.LeakyReLU()(conv8)\n",
    "    conv19 = layers.Conv2D(filters=32, kernel_size=(3, 3), activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv18)\n",
    "    #conv8 = layers.LeakyReLU()(conv8)\n",
    "\n",
    "    up4 = layers.UpSampling2D(size = (2, 2), data_format=None)(conv19)\n",
    "    conv20 = layers.Conv2D(filters=16, kernel_size=(2, 2), activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(up4)\n",
    "    #up9 = layers.LeakyReLU()(up9)\n",
    "    merge4 = layers.concatenate([conv2,conv20], axis = 3)\n",
    "    conv21 = layers.Conv2D(filters=16, kernel_size=(3, 3), activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge4)\n",
    "    #conv9 = layers.LeakyReLU()(conv9)\n",
    "    conv22 = layers.Conv2D(filters=16, kernel_size=(3, 3), activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv21)\n",
    "    #conv9 = layers.LeakyReLU()(conv9)\n",
    "    conv23 = layers.Conv2D(filters=2, kernel_size=(3, 3), activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv22)\n",
    "    #conv9 = layers.LeakyReLU()(conv9)\n",
    "    output = layers.Conv2D(filters=1, kernel_size=(1, 1), activation = 'tanh')(conv23)\n",
    "\n",
    "    model = Model(inputt,output)\n",
    "    model.compile(optimizer = 'adam', loss = tf.keras.losses.Huber(), metrics = ['mae'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(spectrogram_path, weights_path, epoch, batch_size, mode):\n",
    "# train mode and evaluate mode\n",
    "    noisy_voice = np.load(path_save_spectrogram +'noisy_voice_amp_db'+\".npy\")\n",
    "    clean_voice = np.load(path_save_spectrogram +'voice_amp_db'+\".npy\")\n",
    "    \n",
    "    # Simply present the noise as noisyvoice-cleanvoice\n",
    "    noise = noisy_voice - clean_voice\n",
    "\n",
    "    # Normalize the data to [-1, 1]\n",
    "    noisy_voice = (noisy_voice+46)/50\n",
    "    noise = (noise-6)/82\n",
    "\n",
    "    # Reshape\n",
    "    noisy_voice = noisy_voice[:,:,:]\n",
    "    noisy_voice = noisy_voice.reshape(noisy_voice.shape[0],noisy_voice.shape[1],noisy_voice.shape[2],1)\n",
    "    noise = noise[:,:,:]\n",
    "    noise = noise.reshape(noise.shape[0],noise.shape[1],noise.shape[2],1)\n",
    "\n",
    "    # Split the data. The sample proportion is 0.1. Set the random seed=1 to make sure we get the same data each time\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(noisy_voice, noise, test_size=0.10, random_state=1)\n",
    "\n",
    "    model=trainmodel()\n",
    "\n",
    "    # Training\n",
    "    if mode=='Train':\n",
    "        model.summary()\n",
    "        history = model.fit(X_train, \n",
    "                            Y_train, \n",
    "                            epochs=epoch, \n",
    "                            batch_size=batch_size, \n",
    "                            validation_data=(X_test, Y_test), \n",
    "                            shuffle=True, \n",
    "                            verbose=1)\n",
    "\n",
    "        # Plot accuracy vs epoch\n",
    "        plt.figure(figsize=(15, 5))\n",
    "        plt.subplot(121)\n",
    "        plt.plot(history.history['accuracy'])\n",
    "        plt.plot(history.history['val_accuracy'])\n",
    "        plt.title('Model Accuracy')\n",
    "        plt.ylabel('accuracy')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.legend(['Train', 'Test'], loc='lower right',prop={'size':13})\n",
    "\n",
    "        # Plot loss vs epoch\n",
    "        plt.subplot(122)\n",
    "        plt.plot(history.history['loss'])\n",
    "        plt.plot(history.history['val_loss'])\n",
    "        plt.title('Model Loss')\n",
    "        plt.ylabel('loss')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.legend(['Train', 'Test'], loc='upper right',prop={'size':13})\n",
    "        plt.savefig(\"Accuracy&Loss.png\")\n",
    "        plt.show()\n",
    "    if mode=='Evaluate':\n",
    "        pretrained_weights = weights_path+'pretrained_model.h5'\n",
    "        model.load_weights(pretrained_weights)\n",
    "        testacc=model.evaluate(X_test, Y_test, batch_size=32, verbose=0)[1]\n",
    "        print(\"Evaluate accuracy: \", testacc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "\n",
    "def audio_to_audio_frame_stack(sound_data, frame_length, hop_length_frame):\n",
    "    \"\"\"This function take an audio and split into several frame\n",
    "       in a numpy matrix of size (nb_frame,frame_length)\"\"\"\n",
    "\n",
    "    sequence_sample_length = sound_data.shape[0]\n",
    "\n",
    "    sound_data_list = [sound_data[start:start + frame_length] for start in range(\n",
    "    0, sequence_sample_length - frame_length + 1, hop_length_frame)]  # get sliding windows\n",
    "    sound_data_array = np.vstack(sound_data_list)\n",
    "\n",
    "    return sound_data_array\n",
    "\n",
    "\n",
    "def audio_files_to_numpy(audio_dir, list_audio_files, sample_rate, frame_length, hop_length_frame, min_duration):\n",
    "    \"\"\"This function take audio files of a directory and merge them\n",
    "    in a numpy matrix of size (nb_frame,frame_length) for a sliding window of size hop_length_frame\"\"\"\n",
    "\n",
    "    list_sound_array = []\n",
    "\n",
    "    for file in list_audio_files:\n",
    "        # open the audio file\n",
    "        y, sr = librosa.load(os.path.join(audio_dir, file), sr=sample_rate)\n",
    "        total_duration = librosa.get_duration(y=y, sr=sr)\n",
    "\n",
    "        if (total_duration >= min_duration):\n",
    "            list_sound_array.append(audio_to_audio_frame_stack(\n",
    "                y, frame_length, hop_length_frame))\n",
    "        else:\n",
    "            print(\n",
    "                f\"The following file {os.path.join(audio_dir,file)} is below the min duration\")\n",
    "\n",
    "    return np.vstack(list_sound_array)\n",
    "\n",
    "\n",
    "def blend_noise_randomly(voice, noise, nb_samples, frame_length):\n",
    "    \"\"\"This function takes as input numpy arrays representing frames\n",
    "    of voice sounds, noise sounds and the number of frames to be created\n",
    "    and return numpy arrays with voice randomly blend with noise\"\"\"\n",
    "\n",
    "    prod_voice = np.zeros((nb_samples, frame_length))\n",
    "    prod_noise = np.zeros((nb_samples, frame_length))\n",
    "    prod_noisy_voice = np.zeros((nb_samples, frame_length))\n",
    "\n",
    "    for i in range(nb_samples):\n",
    "        id_voice = np.random.randint(0, voice.shape[0])\n",
    "        id_noise = np.random.randint(0, noise.shape[0])\n",
    "        level_noise = np.random.uniform(0.2, 0.8)\n",
    "        prod_voice[i, :] = voice[id_voice, :]\n",
    "        prod_noise[i, :] = level_noise * noise[id_noise, :]\n",
    "        prod_noisy_voice[i, :] = prod_voice[i, :] + prod_noise[i, :]\n",
    "\n",
    "    return prod_voice, prod_noise, prod_noisy_voice\n",
    "\n",
    "\n",
    "def audio_to_magnitude_db_and_phase(n_fft, hop_length_fft, audio):\n",
    "    \"\"\"This function takes an audio and convert into spectrogram,\n",
    "       it returns the magnitude in dB and the phase\"\"\"\n",
    "\n",
    "    stftaudio = librosa.stft(audio, n_fft=n_fft, hop_length=hop_length_fft)\n",
    "    stftaudio_magnitude, stftaudio_phase = librosa.magphase(stftaudio)\n",
    "\n",
    "    stftaudio_magnitude_db = librosa.amplitude_to_db(\n",
    "        stftaudio_magnitude, ref=np.max)\n",
    "\n",
    "    return stftaudio_magnitude_db, stftaudio_phase\n",
    "\n",
    "\n",
    "def numpy_audio_to_matrix_spectrogram(numpy_audio, dim_square_spec, n_fft, hop_length_fft):\n",
    "    \"\"\"This function takes as input a numpy audi of size (nb_frame,frame_length), and return\n",
    "    a numpy containing the matrix spectrogram for amplitude in dB and phase. It will have the size\n",
    "    (nb_frame,dim_square_spec,dim_square_spec)\"\"\"\n",
    "\n",
    "    nb_audio = numpy_audio.shape[0]\n",
    "\n",
    "    m_mag_db = np.zeros((nb_audio, dim_square_spec, dim_square_spec))\n",
    "    m_phase = np.zeros((nb_audio, dim_square_spec, dim_square_spec), dtype=complex)\n",
    "\n",
    "    for i in range(nb_audio):\n",
    "        m_mag_db[i, :, :], m_phase[i, :, :] = audio_to_magnitude_db_and_phase(\n",
    "            n_fft, hop_length_fft, numpy_audio[i])\n",
    "\n",
    "    return m_mag_db, m_phase\n",
    "\n",
    "\n",
    "def magnitude_db_and_phase_to_audio(frame_length, hop_length_fft, stftaudio_magnitude_db, stftaudio_phase):\n",
    "    \"\"\"This functions reverts a spectrogram to an audio\"\"\"\n",
    "\n",
    "    stftaudio_magnitude_rev = librosa.db_to_amplitude(stftaudio_magnitude_db, ref=1.0)\n",
    "\n",
    "    # taking magnitude and phase of audio\n",
    "    audio_reverse_stft = stftaudio_magnitude_rev * stftaudio_phase\n",
    "    audio_reconstruct = librosa.core.istft(audio_reverse_stft, hop_length=hop_length_fft, length=frame_length)\n",
    "\n",
    "    return audio_reconstruct\n",
    "\n",
    "def matrix_spectrogram_to_numpy_audio(m_mag_db, m_phase, frame_length, hop_length_fft)  :\n",
    "    \"\"\"This functions reverts the matrix spectrograms to numpy audio\"\"\"\n",
    "\n",
    "    list_audio = []\n",
    "\n",
    "    nb_spec = m_mag_db.shape[0]\n",
    "\n",
    "    for i in range(nb_spec):\n",
    "\n",
    "        audio_reconstruct = magnitude_db_and_phase_to_audio(frame_length, hop_length_fft, m_mag_db[i], m_phase[i])\n",
    "        list_audio.append(audio_reconstruct)\n",
    "\n",
    "    return np.vstack(list_audio)\n",
    "\n",
    "def scaled_in(matrix_spec):\n",
    "    \"global scaling apply to noisy voice spectrograms (scale between -1 and 1)\"\n",
    "    matrix_spec = (matrix_spec + 46)/50\n",
    "    return matrix_spec\n",
    "\n",
    "def scaled_ou(matrix_spec):\n",
    "    \"global scaling apply to noise models spectrograms (scale between -1 and 1)\"\n",
    "    matrix_spec = (matrix_spec -6 )/82\n",
    "    return matrix_spec\n",
    "\n",
    "def inv_scaled_in(matrix_spec):\n",
    "    \"inverse global scaling apply to noisy voices spectrograms\"\n",
    "    matrix_spec = matrix_spec * 50 - 46\n",
    "    return matrix_spec\n",
    "\n",
    "def inv_scaled_ou(matrix_spec):\n",
    "    \"inverse global scaling apply to noise models spectrograms\"\n",
    "    matrix_spec = matrix_spec * 82 + 6\n",
    "    return matrix_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import librosa\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "def create_data(noise_dir, voice_dir, path_save_time_serie, path_save_sound, path_save_spectrogram, sample_rate,\n",
    "min_duration, frame_length, hop_length_frame, hop_length_frame_noise, nb_samples, n_fft, hop_length_fft):\n",
    "    \"\"\"This function will randomly blend some clean voices from voice_dir with some noises from noise_dir\n",
    "    and save the spectrograms of noisy voice, noise and clean voices to disk as well as complex phase,\n",
    "    time series and sounds. This aims at preparing datasets for denoising training. It takes as inputs\n",
    "    parameters defined in args module\"\"\"\n",
    "\n",
    "    list_noise_files = os.listdir(noise_dir)\n",
    "    list_voice_files = os.listdir(voice_dir)\n",
    "\n",
    "    def remove_ds_store(lst):\n",
    "        \"\"\"remove mac specific file if present\"\"\"\n",
    "        if '.DS_Store' in lst:\n",
    "            lst.remove('.DS_Store')\n",
    "\n",
    "        return lst\n",
    "\n",
    "    list_noise_files = remove_ds_store(list_noise_files)\n",
    "    list_voice_files = remove_ds_store(list_voice_files)\n",
    "\n",
    "    nb_voice_files = len(list_voice_files)\n",
    "    nb_noise_files = len(list_noise_files)\n",
    "\n",
    "\n",
    "    # Extracting noise and voice from folder and convert to numpy\n",
    "    noise = audio_files_to_numpy(noise_dir, list_noise_files, sample_rate,\n",
    "                                     frame_length, hop_length_frame_noise, min_duration)\n",
    "\n",
    "    voice = audio_files_to_numpy(voice_dir, list_voice_files,\n",
    "                                     sample_rate, frame_length, hop_length_frame, min_duration)\n",
    "\n",
    "    # Blend some clean voices with random selected noises (and a random level of noise)\n",
    "    prod_voice, prod_noise, prod_noisy_voice = blend_noise_randomly(\n",
    "            voice, noise, nb_samples, frame_length)\n",
    "\n",
    "    # To save the long audio generated to disk to QC:\n",
    "    noisy_voice_long = prod_noisy_voice.reshape(1, nb_samples * frame_length)\n",
    "    librosa.output.write_wav(path_save_sound + 'noisy_voice_long.wav', noisy_voice_long[0, :], sample_rate)\n",
    "    voice_long = prod_voice.reshape(1, nb_samples * frame_length)\n",
    "    librosa.output.write_wav(path_save_sound + 'voice_long.wav', voice_long[0, :], sample_rate)\n",
    "    noise_long = prod_noise.reshape(1, nb_samples * frame_length)\n",
    "    librosa.output.write_wav(path_save_sound + 'noise_long.wav', noise_long[0, :], sample_rate)\n",
    "\n",
    "    # Squared spectrogram dimensions\n",
    "    dim_square_spec = int(n_fft / 2) + 1\n",
    "\n",
    "    # Create Amplitude and phase of the sounds\n",
    "    m_amp_db_voice,  m_pha_voice = numpy_audio_to_matrix_spectrogram(\n",
    "            prod_voice, dim_square_spec, n_fft, hop_length_fft)\n",
    "    m_amp_db_noise,  m_pha_noise = numpy_audio_to_matrix_spectrogram(\n",
    "            prod_noise, dim_square_spec, n_fft, hop_length_fft)\n",
    "    m_amp_db_noisy_voice,  m_pha_noisy_voice = numpy_audio_to_matrix_spectrogram(\n",
    "            prod_noisy_voice, dim_square_spec, n_fft, hop_length_fft)\n",
    "\n",
    "    # Save to disk for Training / QC\n",
    "    np.save(path_save_time_serie + 'voice_timeserie', prod_voice)\n",
    "    np.save(path_save_time_serie + 'noise_timeserie', prod_noise)\n",
    "    np.save(path_save_time_serie + 'noisy_voice_timeserie', prod_noisy_voice)\n",
    "\n",
    "\n",
    "    np.save(path_save_spectrogram + 'voice_amp_db', m_amp_db_voice)\n",
    "    np.save(path_save_spectrogram + 'noise_amp_db', m_amp_db_noise)\n",
    "    np.save(path_save_spectrogram + 'noisy_voice_amp_db', m_amp_db_noisy_voice)\n",
    "\n",
    "    np.save(path_save_spectrogram + 'voice_pha_db', m_pha_voice)\n",
    "    np.save(path_save_spectrogram + 'noise_pha_db', m_pha_noise)\n",
    "    np.save(path_save_spectrogram + 'noisy_voice_pha_db', m_pha_noisy_voice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import model_from_json\n",
    "\n",
    "\n",
    "def prediction(weights_path, name_model, audio_dir_prediction, dir_save_prediction, audio_input_prediction,\n",
    "audio_output_prediction, sample_rate, min_duration, frame_length, hop_length_frame, n_fft, hop_length_fft):\n",
    "    \"\"\" This function takes as input pretrained weights, noisy voice sound to denoise, predict\n",
    "    the denoise sound and save it to disk.\n",
    "    \"\"\"\n",
    "\n",
    "    # load json and create model\n",
    "    json_file = open(weights_path+'/'+name_model+'.json', 'r')\n",
    "    loaded_model_json = json_file.read()\n",
    "    json_file.close()\n",
    "    loaded_model = model_from_json(loaded_model_json)\n",
    "    # load weights into new model\n",
    "    loaded_model.load_weights(weights_path+'/'+name_model+'.h5')\n",
    "    print(\"Loaded model from disk\")\n",
    "\n",
    "    # Extracting noise and voice from folder and convert to numpy\n",
    "    audio = audio_files_to_numpy(audio_dir_prediction, audio_input_prediction, sample_rate,\n",
    "                                 frame_length, hop_length_frame, min_duration)\n",
    "\n",
    "    #Dimensions of squared spectrogram\n",
    "    dim_square_spec = int(n_fft / 2) + 1\n",
    "    print(dim_square_spec)\n",
    "\n",
    "    # Create Amplitude and phase of the sounds\n",
    "    m_amp_db_audio,  m_pha_audio = numpy_audio_to_matrix_spectrogram(\n",
    "        audio, dim_square_spec, n_fft, hop_length_fft)\n",
    "\n",
    "    #global scaling to have distribution -1/1\n",
    "    X_in = scaled_in(m_amp_db_audio)\n",
    "    #Reshape for prediction\n",
    "    X_in = X_in.reshape(X_in.shape[0],X_in.shape[1],X_in.shape[2],1)\n",
    "    #Prediction using loaded network\n",
    "    X_pred = loaded_model.predict(X_in)\n",
    "    #Rescale back the noise model\n",
    "    inv_sca_X_pred = inv_scaled_ou(X_pred)\n",
    "    #Remove noise model from noisy speech\n",
    "    X_denoise = m_amp_db_audio - inv_sca_X_pred[:,:,:,0]\n",
    "    #Reconstruct audio from denoised spectrogram and phase\n",
    "    print(X_denoise.shape)\n",
    "    print(m_pha_audio.shape)\n",
    "    print(frame_length)\n",
    "    print(hop_length_fft)\n",
    "    audio_denoise_recons = matrix_spectrogram_to_numpy_audio(X_denoise, m_pha_audio, frame_length, hop_length_fft)\n",
    "    #Number of frames\n",
    "    nb_samples = audio_denoise_recons.shape[0]\n",
    "    #Save all frames in one file\n",
    "    denoise_long = audio_denoise_recons.reshape(1, nb_samples * frame_length)*10\n",
    "    librosa.output.write_wav(dir_save_prediction + audio_output_prediction, denoise_long[0, :], sample_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parameters\n",
    "\n",
    "\n",
    "#folders where to find noise audios and clean voice audio to prepare training dataset (mode data_creation)\n",
    "noise_dir = '/noise'\n",
    "\n",
    "voice_dir ='/clean'\n",
    "#folders where to save spectrograms, time series and sounds for training / QC\n",
    "path_save_spectrogram='/spectrogram/'\n",
    "\n",
    "path_save_time_serie = '/time_serie/'\n",
    "\n",
    "path_save_sound = '/sound/'\n",
    "\n",
    "\n",
    "nb_samples = 50\n",
    "\n",
    "training_from_scratch = True\n",
    "\n",
    "weights_folder ='./weights'\n",
    "#Nb of epochs for training\n",
    "epoch = 10\n",
    "#Batch size for training\n",
    "batch_size = 20\n",
    "\n",
    "#directory where read noisy sound to denoise (prediction mode)\n",
    "audio_dir_prediction = './demo_test/'\n",
    "#directory to save the denoise sound (prediction mode)\n",
    "dir_save_prediction = './demo_predictions/'\n",
    "#Noisy sound file to denoise (prediction mode)\n",
    "audio_input_prediction = ['noisy_voice_long_t2.wav']\n",
    "#File name of sound output of denoise prediction\n",
    "audio_output_prediction = 'denoise_t2.wav'\n",
    "\n",
    "sample_rate = 8000\n",
    "# Minimum duration of audio files to consider\n",
    "min_duration = 1.0\n",
    "# Training data will be frame of slightly above 1 second\n",
    "frame_length = 8064\n",
    "# hop length for clean voice files separation (no overlap)\n",
    "hop_length_frame =8064\n",
    "# hop length for noise files to blend (noise is splitted into several windows)\n",
    "hop_length_frame_noise = 5000\n",
    "# Choosing n_fft and hop_length_fft to have squared spectrograms\n",
    "n_fft = 255\n",
    "\n",
    "hop_length_fft = 63"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] 系统找不到指定的路径。: '/noise'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-42-7ba0d38a13a7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mdata_mode\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m         create_data(noise_dir, voice_dir, path_save_time_serie, path_save_sound, path_save_spectrogram, sample_rate,\n\u001b[1;32m---> 21\u001b[1;33m         min_duration, frame_length, hop_length_frame, hop_length_frame_noise, nb_samples, n_fft, hop_length_fft)\n\u001b[0m\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-31-c720ce76ecd3>\u001b[0m in \u001b[0;36mcreate_data\u001b[1;34m(noise_dir, voice_dir, path_save_time_serie, path_save_sound, path_save_spectrogram, sample_rate, min_duration, frame_length, hop_length_frame, hop_length_frame_noise, nb_samples, n_fft, hop_length_fft)\u001b[0m\n\u001b[0;32m     12\u001b[0m     parameters defined in args module\"\"\"\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m     \u001b[0mlist_noise_files\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnoise_dir\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m     \u001b[0mlist_voice_files\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvoice_dir\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] 系统找不到指定的路径。: '/noise'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    mode='data_creation'\n",
    "    # Initialize all modes to zero\n",
    "    data_mode = False\n",
    "    training_mode = False\n",
    "    prediction_mode = False\n",
    "\n",
    "    # Update with the mode the user is asking\n",
    "    if mode == 'prediction':\n",
    "        prediction_mode = True\n",
    "    elif mode == 'training':\n",
    "        training_mode = True\n",
    "    elif mode == 'data_creation':\n",
    "        data_mode = True\n",
    "\n",
    "    if data_mode:\n",
    "        create_data(noise_dir, voice_dir, path_save_time_serie, path_save_sound, path_save_spectrogram, sample_rate,\n",
    "        min_duration, frame_length, hop_length_frame, hop_length_frame_noise, nb_samples, n_fft, hop_length_fft)\n",
    "\n",
    "\n",
    "    elif training_mode:\n",
    "        # mode can be Train or Evaluate\n",
    "        mode='Evaluate'\n",
    "        train(path_save_spectrogram, weights_path, epoch, batch_size, mode)\n",
    "\n",
    "    elif prediction_mode:\n",
    "        prediction(weights_path, name_model, audio_dir_prediction, dir_save_prediction, audio_input_prediction,\n",
    "        audio_output_prediction, sample_rate, min_duration, frame_length, hop_length_frame, n_fft, hop_length_fft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Downloading https://files.pythonhosted.org/packages/af/50/d7da24189d95e2084bb1cc350a8e4acdf1b0c9b3d57def7a348f0d9cb062/tensorflow-2.2.0-cp37-cp37m-win_amd64.whl (459.2MB)\n",
      "Requirement already satisfied, skipping upgrade: absl-py>=0.7.0 in c:\\users\\74019\\anaconda3\\lib\\site-packages (from tensorflow) (0.9.0)\n",
      "Collecting h5py<2.11.0,>=2.10.0 (from tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/a1/6b/7f62017e3f0b32438dd90bdc1ff0b7b1448b6cb04a1ed84f37b6de95cd7b/h5py-2.10.0-cp37-cp37m-win_amd64.whl (2.5MB)\n",
      "Requirement already satisfied, skipping upgrade: numpy<2.0,>=1.16.0 in c:\\users\\74019\\anaconda3\\lib\\site-packages (from tensorflow) (1.16.5)\n",
      "Requirement already satisfied, skipping upgrade: wheel>=0.26; python_version >= \"3\" in c:\\users\\74019\\anaconda3\\lib\\site-packages (from tensorflow) (0.33.6)\n",
      "Requirement already satisfied, skipping upgrade: wrapt>=1.11.1 in c:\\users\\74019\\anaconda3\\lib\\site-packages (from tensorflow) (1.11.2)\n",
      "Requirement already satisfied, skipping upgrade: six>=1.12.0 in c:\\users\\74019\\anaconda3\\lib\\site-packages (from tensorflow) (1.12.0)\n",
      "Requirement already satisfied, skipping upgrade: scipy==1.4.1; python_version >= \"3\" in c:\\users\\74019\\anaconda3\\lib\\site-packages (from tensorflow) (1.4.1)\n",
      "Requirement already satisfied, skipping upgrade: protobuf>=3.8.0 in c:\\users\\74019\\anaconda3\\lib\\site-packages (from tensorflow) (3.11.3)\n",
      "Collecting tensorflow-estimator<2.3.0,>=2.2.0 (from tensorflow)\n",
      "  Using cached https://files.pythonhosted.org/packages/a4/f5/926ae53d6a226ec0fda5208e0e581cffed895ccc89e36ba76a8e60895b78/tensorflow_estimator-2.2.0-py2.py3-none-any.whl\n",
      "Requirement already satisfied, skipping upgrade: termcolor>=1.1.0 in c:\\users\\74019\\anaconda3\\lib\\site-packages (from tensorflow) (1.1.0)\n",
      "Requirement already satisfied, skipping upgrade: opt-einsum>=2.3.2 in c:\\users\\74019\\anaconda3\\lib\\site-packages (from tensorflow) (3.1.0)\n",
      "Requirement already satisfied, skipping upgrade: google-pasta>=0.1.8 in c:\\users\\74019\\anaconda3\\lib\\site-packages (from tensorflow) (0.1.8)\n",
      "Collecting astunparse==1.6.3 (from tensorflow)\n",
      "  Using cached https://files.pythonhosted.org/packages/2b/03/13dde6512ad7b4557eb792fbcf0c653af6076b81e5941d36ec61f7ce6028/astunparse-1.6.3-py2.py3-none-any.whl\n",
      "Collecting gast==0.3.3 (from tensorflow)\n",
      "  Using cached https://files.pythonhosted.org/packages/d6/84/759f5dd23fec8ba71952d97bcc7e2c9d7d63bdc582421f3cd4be845f0c98/gast-0.3.3-py2.py3-none-any.whl\n",
      "Requirement already satisfied, skipping upgrade: keras-preprocessing>=1.1.0 in c:\\users\\74019\\anaconda3\\lib\\site-packages (from tensorflow) (1.1.0)\n",
      "Collecting tensorboard<2.3.0,>=2.2.0 (from tensorflow)\n",
      "  Using cached https://files.pythonhosted.org/packages/1d/fd/4f3ca1516cbb3713259ef229abd9314bba0077ef6070285dde0dd1ed21b2/tensorboard-2.2.1-py3-none-any.whl\n",
      "Requirement already satisfied, skipping upgrade: grpcio>=1.8.6 in c:\\users\\74019\\anaconda3\\lib\\site-packages (from tensorflow) (1.27.2)\n",
      "Requirement already satisfied, skipping upgrade: setuptools in c:\\users\\74019\\anaconda3\\lib\\site-packages (from protobuf>=3.8.0->tensorflow) (41.4.0)\n",
      "Collecting tensorboard-plugin-wit>=1.6.0 (from tensorboard<2.3.0,>=2.2.0->tensorflow)\n",
      "  Using cached https://files.pythonhosted.org/packages/51/cd/a0c1f9e4582ea64dddf76c1b808b318d01e3b858a51c715bffab1016ecc7/tensorboard_plugin_wit-1.6.0.post3-py3-none-any.whl\n",
      "Requirement already satisfied, skipping upgrade: werkzeug>=0.11.15 in c:\\users\\74019\\anaconda3\\lib\\site-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow) (0.16.0)\n",
      "Requirement already satisfied, skipping upgrade: markdown>=2.6.8 in c:\\users\\74019\\anaconda3\\lib\\site-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow) (3.2.1)\n",
      "Requirement already satisfied, skipping upgrade: google-auth-oauthlib<0.5,>=0.4.1 in c:\\users\\74019\\anaconda3\\lib\\site-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow) (0.4.1)\n",
      "Requirement already satisfied, skipping upgrade: google-auth<2,>=1.6.3 in c:\\users\\74019\\anaconda3\\lib\\site-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow) (1.11.2)\n",
      "Requirement already satisfied, skipping upgrade: requests<3,>=2.21.0 in c:\\users\\74019\\anaconda3\\lib\\site-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow) (2.22.0)\n",
      "Requirement already satisfied, skipping upgrade: requests-oauthlib>=0.7.0 in c:\\users\\74019\\anaconda3\\lib\\site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.3.0,>=2.2.0->tensorflow) (1.3.0)\n",
      "Requirement already satisfied, skipping upgrade: pyasn1-modules>=0.2.1 in c:\\users\\74019\\anaconda3\\lib\\site-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow) (0.2.8)\n",
      "Requirement already satisfied, skipping upgrade: cachetools<5.0,>=2.0.0 in c:\\users\\74019\\anaconda3\\lib\\site-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow) (4.0.0)\n",
      "Requirement already satisfied, skipping upgrade: rsa<4.1,>=3.1.4 in c:\\users\\74019\\anaconda3\\lib\\site-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow) (4.0)\n",
      "Requirement already satisfied, skipping upgrade: idna<2.9,>=2.5 in c:\\users\\74019\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow) (2.8)\n",
      "Requirement already satisfied, skipping upgrade: chardet<3.1.0,>=3.0.2 in c:\\users\\74019\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow) (3.0.4)\n",
      "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in c:\\users\\74019\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow) (2019.11.28)\n",
      "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\74019\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow) (1.24.2)\n",
      "Requirement already satisfied, skipping upgrade: oauthlib>=3.0.0 in c:\\users\\74019\\anaconda3\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.3.0,>=2.2.0->tensorflow) (3.1.0)\n",
      "Requirement already satisfied, skipping upgrade: pyasn1<0.5.0,>=0.4.6 in c:\\users\\74019\\anaconda3\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow) (0.4.8)\n",
      "Installing collected packages: h5py, tensorflow-estimator, astunparse, gast, tensorboard-plugin-wit, tensorboard, tensorflow\n",
      "  Found existing installation: h5py 2.9.0\n",
      "    Uninstalling h5py-2.9.0:\n",
      "      Successfully uninstalled h5py-2.9.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not install packages due to an EnvironmentError: [WinError 5] 拒绝访问。: 'c:\\\\users\\\\74019\\\\anaconda3\\\\lib\\\\site-packages\\\\~5py\\\\defs.cp37-win_amd64.pyd'\n",
      "Consider using the `--user` option or check the permissions.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
