{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting datatools\n",
      "  Downloading https://files.pythonhosted.org/packages/39/1a/41111239f51eb3c75505c314af8c251acf49c806d20314eaf0d5526261b9/datatools-0.1.2.tar.gz\n",
      "Building wheels for collected packages: datatools\n",
      "  Building wheel for datatools (setup.py): started\n",
      "  Building wheel for datatools (setup.py): finished with status 'done'\n",
      "  Created wheel for datatools: filename=datatools-0.1.2-cp36-none-any.whl size=9426 sha256=6f80cf92fb5a6942410aa036a2e1cfd252701f798c936d180de8d063e1a08220\n",
      "  Stored in directory: C:\\Users\\vetala\\AppData\\Local\\pip\\Cache\\wheels\\8e\\27\\d0\\85e0e5e87f42bab2df0f1fc9defe375b9fb12f0acb5a531c17\n",
      "Successfully built datatools\n",
      "Installing collected packages: datatools\n",
      "Successfully installed datatools-0.1.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 19.3.1; however, version 20.1.1 is available.\n",
      "You should consider upgrading via the 'python -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "pip install datatools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.12.0\n"
     ]
    }
   ],
   "source": [
    "# Check that TF 2.1.0 is in use\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainmodel():\n",
    "    inputt = layers.Input((128,128,1))\n",
    "    conv1 = layers.Conv2D(filters=16, kernel_size=(3, 3), activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(inputt)\n",
    "    #conv1 = layers.LeakyReLU()(conv1)\n",
    "    conv2 = layers.Conv2D(filters=16, kernel_size=(3, 3), activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv1)\n",
    "    #conv1 = layers.LeakyReLU()(conv1)\n",
    "    pool1 = layers.MaxPooling2D(pool_size=(2, 2), strides=None, padding='valid', data_format=None)(conv2)\n",
    "    conv3 = layers.Conv2D(filters=32, kernel_size=(3, 3), activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool1)\n",
    "    #conv2 = layers.LeakyReLU()(conv2)\n",
    "    conv4 = layers.Conv2D(filters=32, kernel_size=(3, 3), activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv3)\n",
    "    #conv2 = layers.LeakyReLU()(conv2)\n",
    "    pool2 = layers.MaxPooling2D(pool_size=(2, 2), strides=None, padding='valid', data_format=None)(conv4)\n",
    "    conv5 = layers.Conv2D(filters=64, kernel_size=(3, 3), activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool2)\n",
    "    #conv3 = layers.LeakyReLU()(conv3)\n",
    "    conv6 = layers.Conv2D(filters=64, kernel_size=(3, 3), activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv5)\n",
    "    #conv3 = layers.LeakyReLU()(conv3)\n",
    "    pool3 = layers.MaxPooling2D(pool_size=(2, 2), strides=None, padding='valid', data_format=None)(conv6)\n",
    "    conv7 = layers.Conv2D(filters=128, kernel_size=(3, 3), activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool3)\n",
    "    #conv4 = layers.LeakyReLU()(conv4)\n",
    "    conv8 = layers.Conv2D(filters=128, kernel_size=(3, 3), activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv7)\n",
    "    #conv4 = layers.LeakyReLU()(conv4)\n",
    "    drop1 = layers.Dropout(0.5)(conv8)\n",
    "    pool4 = layers.MaxPooling2D(pool_size=(2, 2), strides=None, padding='valid', data_format=None)(drop1)\n",
    "\n",
    "    conv9 = layers.Conv2D(filters=256, kernel_size=(3, 3), activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool4)\n",
    "    #conv5 = layers.LeakyReLU()(conv5)\n",
    "    conv10 = layers.Conv2D(filters=256, kernel_size=(3, 3), activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv9)\n",
    "    #conv5 = layers.LeakyReLU()(conv5)\n",
    "    drop2 = layers.Dropout(0.5)(conv10)\n",
    "    \n",
    "    up1 = layers.UpSampling2D(size = (2, 2), data_format=None)(drop2)\n",
    "    conv11 = layers.Conv2D(filters=128, kernel_size=(2, 2), activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(up1)\n",
    "    #up6 = layers.LeakyReLU()(up6)\n",
    "    merge1 = layers.concatenate([drop1,conv11], axis = 3)\n",
    "    conv12 = layers.Conv2D(filters=128, kernel_size=(3, 3), activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge1)\n",
    "    #conv6 = layers.LeakyReLU()(conv6)\n",
    "    conv13 = layers.Conv2D(filters=128, kernel_size=(3, 3), activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv12)\n",
    "    #conv6 = layers.LeakyReLU()(conv6)\n",
    "    up2 = layers.UpSampling2D(size = (2, 2), data_format=None)(conv13)\n",
    "    conv14 = layers.Conv2D(filters=64, kernel_size=(2, 2), activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(up2)\n",
    "    #up7 = layers.LeakyReLU()(up7)\n",
    "    merge2 = layers.concatenate([conv6,conv14], axis = 3)\n",
    "    conv15 = layers.Conv2D(filters=64, kernel_size=(3, 3), activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge2)\n",
    "    #conv7 = layers.LeakyReLU()(conv7)\n",
    "    conv16 = layers.Conv2D(filters=64, kernel_size=(3, 3), activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv15)\n",
    "    #conv7 = layers.LeakyReLU()(conv7)\n",
    "    up3 = layers.UpSampling2D(size = (2, 2), data_format=None)(conv16)\n",
    "    conv17 = layers.Conv2D(filters=32, kernel_size=(2, 2), activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(up3)\n",
    "    #up8 = layers.LeakyReLU()(up8)\n",
    "    merge3 = layers.concatenate([conv4,conv17], axis = 3)\n",
    "    conv18 = layers.Conv2D(filters=32, kernel_size=(3, 3), activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge3)\n",
    "    #conv8 = layers.LeakyReLU()(conv8)\n",
    "    conv19 = layers.Conv2D(filters=32, kernel_size=(3, 3), activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv18)\n",
    "    #conv8 = layers.LeakyReLU()(conv8)\n",
    "\n",
    "    up4 = layers.UpSampling2D(size = (2, 2), data_format=None)(conv19)\n",
    "    conv20 = layers.Conv2D(filters=16, kernel_size=(2, 2), activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(up4)\n",
    "    #up9 = layers.LeakyReLU()(up9)\n",
    "    merge4 = layers.concatenate([conv2,conv20], axis = 3)\n",
    "    conv21 = layers.Conv2D(filters=16, kernel_size=(3, 3), activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge4)\n",
    "    #conv9 = layers.LeakyReLU()(conv9)\n",
    "    conv22 = layers.Conv2D(filters=16, kernel_size=(3, 3), activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv21)\n",
    "    #conv9 = layers.LeakyReLU()(conv9)\n",
    "    conv23 = layers.Conv2D(filters=2, kernel_size=(3, 3), activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv22)\n",
    "    #conv9 = layers.LeakyReLU()(conv9)\n",
    "    output = layers.Conv2D(filters=1, kernel_size=(1, 1), activation = 'tanh')(conv23)\n",
    "\n",
    "    model = Model(inputt,output)\n",
    "\n",
    "    model.compile(optimizer = 'adam', loss = tf.keras.losses.Huber(), metrics = ['mae'])\n",
    "\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(path_save_spectrogram, weights_path, name_model, training_from_scratch, epoch, batch_size):\n",
    "\n",
    "    noisy_voice = np.load(path_save_spectrogram +'noisy_voice_amp_db'+\".npy\")\n",
    "    clean_voice = np.load(path_save_spectrogram +'voice_amp_db'+\".npy\")\n",
    "    \n",
    "    # Simply present the noise as noisyvoice-cleanvoice\n",
    "    noise = noisy_voice - clean_voice\n",
    "\n",
    "    # Normalize the data to [-1, 1]\n",
    "    noisy_voice = (noisy_voice+46)/50\n",
    "    noise = (noise-6)/82\n",
    "\n",
    "    # Reshape\n",
    "    noisy_voice = noisy_voice[:,:,:]\n",
    "    noisy_voice = noisy_voice.reshape(noisy_voice.shape[0],noisy_voice.shape[1],noisy_voice.shape[2],1)\n",
    "    noise = noise[:,:,:]\n",
    "    noise = noise.reshape(noise.shape[0],noise.shape[1],noise.shape[2],1)\n",
    "\n",
    "    # Split the data. The sample proportion is 0.1. Set the random seed=1 to make sure we get the same data each time\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(noisy_voice, noise, test_size=0.10, random_state=1)\n",
    "\n",
    "    model=trainmodel()\n",
    "\n",
    "    # Training\n",
    "    history = model.fit(X_train, \n",
    "                        Y_train, \n",
    "                        epochs=epoch, \n",
    "                        batch_size=batch_size, \n",
    "                        validation_data=(X_test, Y_test), \n",
    "                        shuffle=True, \n",
    "                        verbose=1)\n",
    "\n",
    "    # Plot accuracy vs epoch\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    plt.subplot(121)\n",
    "    plt.plot(history.history['accuracy'])\n",
    "    plt.plot(history.history['val_accuracy'])\n",
    "    plt.title('Model Accuracy')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Test'], loc='lower right',prop={'size':13})\n",
    "\n",
    "    # Plot loss vs epoch\n",
    "    plt.subplot(122)\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('Model Loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Test'], loc='upper right',prop={'size':13})\n",
    "    plt.savefig(\"Accuracy&Loss.png\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "\n",
    "def audio_to_audio_frame_stack(sound_data, frame_length, hop_length_frame):\n",
    "    \"\"\"This function take an audio and split into several frame\n",
    "       in a numpy matrix of size (nb_frame,frame_length)\"\"\"\n",
    "\n",
    "    sequence_sample_length = sound_data.shape[0]\n",
    "\n",
    "    sound_data_list = [sound_data[start:start + frame_length] for start in range(\n",
    "    0, sequence_sample_length - frame_length + 1, hop_length_frame)]  # get sliding windows\n",
    "    sound_data_array = np.vstack(sound_data_list)\n",
    "\n",
    "    return sound_data_array\n",
    "\n",
    "\n",
    "def audio_files_to_numpy(audio_dir, list_audio_files, sample_rate, frame_length, hop_length_frame, min_duration):\n",
    "    \"\"\"This function take audio files of a directory and merge them\n",
    "    in a numpy matrix of size (nb_frame,frame_length) for a sliding window of size hop_length_frame\"\"\"\n",
    "\n",
    "    list_sound_array = []\n",
    "\n",
    "    for file in list_audio_files:\n",
    "        # open the audio file\n",
    "        y, sr = librosa.load(os.path.join(audio_dir, file), sr=sample_rate)\n",
    "        total_duration = librosa.get_duration(y=y, sr=sr)\n",
    "\n",
    "        if (total_duration >= min_duration):\n",
    "            list_sound_array.append(audio_to_audio_frame_stack(\n",
    "                y, frame_length, hop_length_frame))\n",
    "        else:\n",
    "            print(\n",
    "                f\"The following file {os.path.join(audio_dir,file)} is below the min duration\")\n",
    "\n",
    "    return np.vstack(list_sound_array)\n",
    "\n",
    "\n",
    "def blend_noise_randomly(voice, noise, nb_samples, frame_length):\n",
    "    \"\"\"This function takes as input numpy arrays representing frames\n",
    "    of voice sounds, noise sounds and the number of frames to be created\n",
    "    and return numpy arrays with voice randomly blend with noise\"\"\"\n",
    "\n",
    "    prod_voice = np.zeros((nb_samples, frame_length))\n",
    "    prod_noise = np.zeros((nb_samples, frame_length))\n",
    "    prod_noisy_voice = np.zeros((nb_samples, frame_length))\n",
    "\n",
    "    for i in range(nb_samples):\n",
    "        id_voice = np.random.randint(0, voice.shape[0])\n",
    "        id_noise = np.random.randint(0, noise.shape[0])\n",
    "        level_noise = np.random.uniform(0.2, 0.8)\n",
    "        prod_voice[i, :] = voice[id_voice, :]\n",
    "        prod_noise[i, :] = level_noise * noise[id_noise, :]\n",
    "        prod_noisy_voice[i, :] = prod_voice[i, :] + prod_noise[i, :]\n",
    "\n",
    "    return prod_voice, prod_noise, prod_noisy_voice\n",
    "\n",
    "\n",
    "def audio_to_magnitude_db_and_phase(n_fft, hop_length_fft, audio):\n",
    "    \"\"\"This function takes an audio and convert into spectrogram,\n",
    "       it returns the magnitude in dB and the phase\"\"\"\n",
    "\n",
    "    stftaudio = librosa.stft(audio, n_fft=n_fft, hop_length=hop_length_fft)\n",
    "    stftaudio_magnitude, stftaudio_phase = librosa.magphase(stftaudio)\n",
    "\n",
    "    stftaudio_magnitude_db = librosa.amplitude_to_db(\n",
    "        stftaudio_magnitude, ref=np.max)\n",
    "\n",
    "    return stftaudio_magnitude_db, stftaudio_phase\n",
    "\n",
    "\n",
    "def numpy_audio_to_matrix_spectrogram(numpy_audio, dim_square_spec, n_fft, hop_length_fft):\n",
    "    \"\"\"This function takes as input a numpy audi of size (nb_frame,frame_length), and return\n",
    "    a numpy containing the matrix spectrogram for amplitude in dB and phase. It will have the size\n",
    "    (nb_frame,dim_square_spec,dim_square_spec)\"\"\"\n",
    "\n",
    "    nb_audio = numpy_audio.shape[0]\n",
    "\n",
    "    m_mag_db = np.zeros((nb_audio, dim_square_spec, dim_square_spec))\n",
    "    m_phase = np.zeros((nb_audio, dim_square_spec, dim_square_spec), dtype=complex)\n",
    "\n",
    "    for i in range(nb_audio):\n",
    "        m_mag_db[i, :, :], m_phase[i, :, :] = audio_to_magnitude_db_and_phase(\n",
    "            n_fft, hop_length_fft, numpy_audio[i])\n",
    "\n",
    "    return m_mag_db, m_phase\n",
    "\n",
    "\n",
    "def magnitude_db_and_phase_to_audio(frame_length, hop_length_fft, stftaudio_magnitude_db, stftaudio_phase):\n",
    "    \"\"\"This functions reverts a spectrogram to an audio\"\"\"\n",
    "\n",
    "    stftaudio_magnitude_rev = librosa.db_to_amplitude(stftaudio_magnitude_db, ref=1.0)\n",
    "\n",
    "    # taking magnitude and phase of audio\n",
    "    audio_reverse_stft = stftaudio_magnitude_rev * stftaudio_phase\n",
    "    audio_reconstruct = librosa.core.istft(audio_reverse_stft, hop_length=hop_length_fft, length=frame_length)\n",
    "\n",
    "    return audio_reconstruct\n",
    "\n",
    "def matrix_spectrogram_to_numpy_audio(m_mag_db, m_phase, frame_length, hop_length_fft)  :\n",
    "    \"\"\"This functions reverts the matrix spectrograms to numpy audio\"\"\"\n",
    "\n",
    "    list_audio = []\n",
    "\n",
    "    nb_spec = m_mag_db.shape[0]\n",
    "\n",
    "    for i in range(nb_spec):\n",
    "\n",
    "        audio_reconstruct = magnitude_db_and_phase_to_audio(frame_length, hop_length_fft, m_mag_db[i], m_phase[i])\n",
    "        list_audio.append(audio_reconstruct)\n",
    "\n",
    "    return np.vstack(list_audio)\n",
    "\n",
    "def scaled_in(matrix_spec):\n",
    "    \"global scaling apply to noisy voice spectrograms (scale between -1 and 1)\"\n",
    "    matrix_spec = (matrix_spec + 46)/50\n",
    "    return matrix_spec\n",
    "\n",
    "def scaled_ou(matrix_spec):\n",
    "    \"global scaling apply to noise models spectrograms (scale between -1 and 1)\"\n",
    "    matrix_spec = (matrix_spec -6 )/82\n",
    "    return matrix_spec\n",
    "\n",
    "def inv_scaled_in(matrix_spec):\n",
    "    \"inverse global scaling apply to noisy voices spectrograms\"\n",
    "    matrix_spec = matrix_spec * 50 - 46\n",
    "    return matrix_spec\n",
    "\n",
    "def inv_scaled_ou(matrix_spec):\n",
    "    \"inverse global scaling apply to noise models spectrograms\"\n",
    "    matrix_spec = matrix_spec * 82 + 6\n",
    "    return matrix_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import librosa\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "def create_data(noise_dir, voice_dir, path_save_time_serie, path_save_sound, path_save_spectrogram, sample_rate,\n",
    "min_duration, frame_length, hop_length_frame, hop_length_frame_noise, nb_samples, n_fft, hop_length_fft):\n",
    "    \"\"\"This function will randomly blend some clean voices from voice_dir with some noises from noise_dir\n",
    "    and save the spectrograms of noisy voice, noise and clean voices to disk as well as complex phase,\n",
    "    time series and sounds. This aims at preparing datasets for denoising training. It takes as inputs\n",
    "    parameters defined in args module\"\"\"\n",
    "\n",
    "    list_noise_files = os.listdir(noise_dir)\n",
    "    list_voice_files = os.listdir(voice_dir)\n",
    "\n",
    "    def remove_ds_store(lst):\n",
    "        \"\"\"remove mac specific file if present\"\"\"\n",
    "        if '.DS_Store' in lst:\n",
    "            lst.remove('.DS_Store')\n",
    "\n",
    "        return lst\n",
    "\n",
    "    list_noise_files = remove_ds_store(list_noise_files)\n",
    "    list_voice_files = remove_ds_store(list_voice_files)\n",
    "\n",
    "    nb_voice_files = len(list_voice_files)\n",
    "    nb_noise_files = len(list_noise_files)\n",
    "\n",
    "\n",
    "    # Extracting noise and voice from folder and convert to numpy\n",
    "    noise = audio_files_to_numpy(noise_dir, list_noise_files, sample_rate,\n",
    "                                     frame_length, hop_length_frame_noise, min_duration)\n",
    "\n",
    "    voice = audio_files_to_numpy(voice_dir, list_voice_files,\n",
    "                                     sample_rate, frame_length, hop_length_frame, min_duration)\n",
    "\n",
    "    # Blend some clean voices with random selected noises (and a random level of noise)\n",
    "    prod_voice, prod_noise, prod_noisy_voice = blend_noise_randomly(\n",
    "            voice, noise, nb_samples, frame_length)\n",
    "\n",
    "    # To save the long audio generated to disk to QC:\n",
    "    noisy_voice_long = prod_noisy_voice.reshape(1, nb_samples * frame_length)\n",
    "    librosa.output.write_wav(path_save_sound + 'noisy_voice_long.wav', noisy_voice_long[0, :], sample_rate)\n",
    "    voice_long = prod_voice.reshape(1, nb_samples * frame_length)\n",
    "    librosa.output.write_wav(path_save_sound + 'voice_long.wav', voice_long[0, :], sample_rate)\n",
    "    noise_long = prod_noise.reshape(1, nb_samples * frame_length)\n",
    "    librosa.output.write_wav(path_save_sound + 'noise_long.wav', noise_long[0, :], sample_rate)\n",
    "\n",
    "    # Squared spectrogram dimensions\n",
    "    dim_square_spec = int(n_fft / 2) + 1\n",
    "\n",
    "    # Create Amplitude and phase of the sounds\n",
    "    m_amp_db_voice,  m_pha_voice = numpy_audio_to_matrix_spectrogram(\n",
    "            prod_voice, dim_square_spec, n_fft, hop_length_fft)\n",
    "    m_amp_db_noise,  m_pha_noise = numpy_audio_to_matrix_spectrogram(\n",
    "            prod_noise, dim_square_spec, n_fft, hop_length_fft)\n",
    "    m_amp_db_noisy_voice,  m_pha_noisy_voice = numpy_audio_to_matrix_spectrogram(\n",
    "            prod_noisy_voice, dim_square_spec, n_fft, hop_length_fft)\n",
    "\n",
    "    # Save to disk for Training / QC\n",
    "    np.save(path_save_time_serie + 'voice_timeserie', prod_voice)\n",
    "    np.save(path_save_time_serie + 'noise_timeserie', prod_noise)\n",
    "    np.save(path_save_time_serie + 'noisy_voice_timeserie', prod_noisy_voice)\n",
    "\n",
    "\n",
    "    np.save(path_save_spectrogram + 'voice_amp_db', m_amp_db_voice)\n",
    "    np.save(path_save_spectrogram + 'noise_amp_db', m_amp_db_noise)\n",
    "    np.save(path_save_spectrogram + 'noisy_voice_amp_db', m_amp_db_noisy_voice)\n",
    "\n",
    "    np.save(path_save_spectrogram + 'voice_pha_db', m_pha_voice)\n",
    "    np.save(path_save_spectrogram + 'noise_pha_db', m_pha_noise)\n",
    "    np.save(path_save_spectrogram + 'noisy_voice_pha_db', m_pha_noisy_voice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import model_from_json\n",
    "\n",
    "\n",
    "def prediction(weights_path, name_model, audio_dir_prediction, dir_save_prediction, audio_input_prediction,\n",
    "audio_output_prediction, sample_rate, min_duration, frame_length, hop_length_frame, n_fft, hop_length_fft):\n",
    "    \"\"\" This function takes as input pretrained weights, noisy voice sound to denoise, predict\n",
    "    the denoise sound and save it to disk.\n",
    "    \"\"\"\n",
    "\n",
    "    # load json and create model\n",
    "    json_file = open(weights_path+'/'+name_model+'.json', 'r')\n",
    "    loaded_model_json = json_file.read()\n",
    "    json_file.close()\n",
    "    loaded_model = model_from_json(loaded_model_json)\n",
    "    # load weights into new model\n",
    "    loaded_model.load_weights(weights_path+'/'+name_model+'.h5')\n",
    "    print(\"Loaded model from disk\")\n",
    "\n",
    "    # Extracting noise and voice from folder and convert to numpy\n",
    "    audio = audio_files_to_numpy(audio_dir_prediction, audio_input_prediction, sample_rate,\n",
    "                                 frame_length, hop_length_frame, min_duration)\n",
    "\n",
    "    #Dimensions of squared spectrogram\n",
    "    dim_square_spec = int(n_fft / 2) + 1\n",
    "    print(dim_square_spec)\n",
    "\n",
    "    # Create Amplitude and phase of the sounds\n",
    "    m_amp_db_audio,  m_pha_audio = numpy_audio_to_matrix_spectrogram(\n",
    "        audio, dim_square_spec, n_fft, hop_length_fft)\n",
    "\n",
    "    #global scaling to have distribution -1/1\n",
    "    X_in = scaled_in(m_amp_db_audio)\n",
    "    #Reshape for prediction\n",
    "    X_in = X_in.reshape(X_in.shape[0],X_in.shape[1],X_in.shape[2],1)\n",
    "    #Prediction using loaded network\n",
    "    X_pred = loaded_model.predict(X_in)\n",
    "    #Rescale back the noise model\n",
    "    inv_sca_X_pred = inv_scaled_ou(X_pred)\n",
    "    #Remove noise model from noisy speech\n",
    "    X_denoise = m_amp_db_audio - inv_sca_X_pred[:,:,:,0]\n",
    "    #Reconstruct audio from denoised spectrogram and phase\n",
    "    print(X_denoise.shape)\n",
    "    print(m_pha_audio.shape)\n",
    "    print(frame_length)\n",
    "    print(hop_length_fft)\n",
    "    audio_denoise_recons = matrix_spectrogram_to_numpy_audio(X_denoise, m_pha_audio, frame_length, hop_length_fft)\n",
    "    #Number of frames\n",
    "    nb_samples = audio_denoise_recons.shape[0]\n",
    "    #Save all frames in one file\n",
    "    denoise_long = audio_denoise_recons.reshape(1, nb_samples * frame_length)*10\n",
    "    librosa.output.write_wav(dir_save_prediction + audio_output_prediction, denoise_long[0, :], sample_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parameters\n",
    "\n",
    "mode = 'prediction'\n",
    "#folders where to find noise audios and clean voice audio to prepare training dataset (mode data_creation)\n",
    "noise_dir = '/noise'\n",
    "\n",
    "voice_dir ='/clean'\n",
    "#folders where to save spectrograms, time series and sounds for training / QC\n",
    "path_save_spectrogram='/spectrogram/'\n",
    "\n",
    "path_save_time_serie = '/time_serie/'\n",
    "\n",
    "path_save_sound = '/sound/'\n",
    "\n",
    "\n",
    "nb_samples = 50\n",
    "\n",
    "training_from_scratch = True\n",
    "\n",
    "weights_folder ='./weights'\n",
    "#Nb of epochs for training\n",
    "epochs = 10\n",
    "#Batch size for training\n",
    "batch_size = 20\n",
    "#Name of saved model to read\n",
    "name_model = 'model_unet'\n",
    "#directory where read noisy sound to denoise (prediction mode)\n",
    "audio_dir_prediction = './demo_test/'\n",
    "#directory to save the denoise sound (prediction mode)\n",
    "dir_save_prediction = './demo_predictions/'\n",
    "#Noisy sound file to denoise (prediction mode)\n",
    "audio_input_prediction = ['noisy_voice_long_t2.wav']\n",
    "#File name of sound output of denoise prediction\n",
    "audio_output_prediction = 'denoise_t2.wav'\n",
    "\n",
    "sample_rate = 8000\n",
    "# Minimum duration of audio files to consider\n",
    "min_duration = 1.0\n",
    "# Training data will be frame of slightly above 1 second\n",
    "frame_length = 8064\n",
    "# hop length for clean voice files separation (no overlap)\n",
    "hop_length_frame =8064\n",
    "# hop length for noise files to blend (noise is splitted into several windows)\n",
    "hop_length_frame_noise = 5000\n",
    "# Choosing n_fft and hop_length_fft to have squared spectrograms\n",
    "n_fft = 255\n",
    "\n",
    "hop_length_fft = 63"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from disk\n",
      "128\n",
      "(5, 128, 128)\n",
      "(5, 128, 128)\n",
      "8064\n",
      "63\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "\n",
    "    # Initialize all modes to zero\n",
    "    data_mode = False\n",
    "    training_mode = False\n",
    "    prediction_mode = False\n",
    "\n",
    "    # Update with the mode the user is asking\n",
    "    if mode == 'prediction':\n",
    "        prediction_mode = True\n",
    "    elif mode == 'training':\n",
    "        training_mode = True\n",
    "    elif mode == 'data_creation':\n",
    "        data_mode = True\n",
    "\n",
    "    if data_mode:\n",
    "        #Example: python main.py --mode='data_creation'\n",
    "\n",
    "        #folder containing noises\n",
    "        noise_dir = noise_dir\n",
    "        #folder containing clean voices\n",
    "        voice_dir = voice_dir\n",
    "        #path to save time series\n",
    "        path_save_time_serie = path_save_time_serie\n",
    "        #path to save sounds\n",
    "        path_save_sound = path_save_sound\n",
    "        #path to save spectrograms\n",
    "        path_save_spectrogram = path_save_spectrogram\n",
    "        # Sample rate to read audio\n",
    "        sample_rate = sample_rate\n",
    "        # Minimum duration of audio files to consider\n",
    "        min_duration = min_duration\n",
    "        #Frame length for training data\n",
    "        frame_length = frame_length\n",
    "        # hop length for clean voice files\n",
    "        hop_length_frame = hop_length_frame\n",
    "        # hop length for noise files\n",
    "        hop_length_frame_noise = hop_length_frame_noise\n",
    "        # How much frame to create for training\n",
    "        nb_samples = nb_samples\n",
    "        #nb of points for fft(for spectrogram computation)\n",
    "        n_fft = n_fft\n",
    "        #hop length for fft\n",
    "        hop_length_fft = hop_length_fft\n",
    "\n",
    "        create_data(noise_dir, voice_dir, path_save_time_serie, path_save_sound, path_save_spectrogram, sample_rate,\n",
    "        min_duration, frame_length, hop_length_frame, hop_length_frame_noise, nb_samples, n_fft, hop_length_fft)\n",
    "\n",
    "\n",
    "    elif training_mode:\n",
    "        #Example: python main.py --mode=\"training\"\n",
    "        #Path were to read spectrograms of noisy voice and clean voice\n",
    "        path_save_spectrogram = path_save_spectrogram\n",
    "        #path to find pre-trained weights / save models\n",
    "        weights_path = weights_folder\n",
    "        #pre trained model\n",
    "        name_model = name_model\n",
    "        #Training from scratch vs training from pre-trained weights\n",
    "        training_from_scratch = training_from_scratch\n",
    "        #epochs for training\n",
    "        epochs = epochs\n",
    "        #batch size for training\n",
    "        batch_size = batch_size\n",
    "\n",
    "        training(path_save_spectrogram, weights_path, name_model, training_from_scratch, epochs, batch_size)\n",
    "\n",
    "    elif prediction_mode:\n",
    "        #Example: python main.py --mode=\"prediction\"\n",
    "        #path to find pre-trained weights / save models\n",
    "        weights_path = weights_folder\n",
    "        #pre trained model\n",
    "        name_model = name_model\n",
    "        #directory where read noisy sound to denoise\n",
    "        audio_dir_prediction = audio_dir_prediction\n",
    "        #directory to save the denoise sound\n",
    "        dir_save_prediction = dir_save_prediction\n",
    "        #Name noisy sound file to denoise\n",
    "        audio_input_prediction = audio_input_prediction\n",
    "        #Name of denoised sound file to save\n",
    "        audio_output_prediction = audio_output_prediction\n",
    "        # Sample rate to read audio\n",
    "        sample_rate = sample_rate\n",
    "        # Minimum duration of audio files to consider\n",
    "        min_duration = min_duration\n",
    "        #Frame length for training data\n",
    "        frame_length = frame_length\n",
    "        # hop length for sound files\n",
    "        hop_length_frame = hop_length_frame\n",
    "        #nb of points for fft(for spectrogram computation)\n",
    "        n_fft = n_fft\n",
    "        #hop length for fft\n",
    "        hop_length_fft = hop_length_fft\n",
    "\n",
    "        prediction(weights_path, name_model, audio_dir_prediction, dir_save_prediction, audio_input_prediction,\n",
    "        audio_output_prediction, sample_rate, min_duration, frame_length, hop_length_frame, n_fft, hop_length_fft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install --upgrade tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
